---
title: "machine_learning_project"
author: "Yue Li"
date: "January 21, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


***Background***

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).


***Data***

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv



***Data Preparation***

First, we follow the instructions to download training and testing data from the provided url and then clean the data by removing invalid independent variables that have near zero values or mostly NA/blank:

```{r}
library(lattice)
library(ggplot2)
library(caret)
library(e1071)

# set the URL for the download
UrlTrain <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
UrlTest  <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

# download the datasets
training <- read.csv(url(UrlTrain))
testing  <- read.csv(url(UrlTest))

# clean data & remove irrelevent predictors
NZV <- nearZeroVar(training)
training_clean <- training[, -NZV]
AllNA <- sapply(training_clean,function(x) mean(is.na(x) | x == "")) > 0.95
training_clean <- training_clean[, AllNA==FALSE]
training_clean <- training_clean[,-c(1:5)]
dim(training_clean)
```

Now we have 19622 records and 54 independent variables in the training data. To compare the model performance created on the training data, we separate out a validation data subset from the training data:

```{r}
# split the training data into a training set and a validation set
set.seed(1234)
inTrain <- createDataPartition(training_clean$classe, p = 0.6, list = FALSE)
trainset <- training_clean[inTrain,]
validset <- training_clean[-inTrain,]
dim(trainset)
dim(validset)
```


***Prediction Models***

Three methods are considered in building prediction models: random forest, generallized boosting model and support vector machine. The best model with highest accuracy rate in validation dataset would be used in final prediction quiz.

**1) Random Forest:**

```{r}
# fit model using random forest
set.seed(2345)
controlrf <- trainControl(method="cv",number=5,verboseIter=FALSE)
mod_rf <- train(classe ~ ., data = trainset, method = "rf",trControl=controlrf)
mod_rf$finalModel

#predict on validation dataset
pred_rf <- predict(mod_rf, validset)

#Accuracy on validation dataset
confusionMatrix(pred_rf, validset$classe)
confusionMatrix(pred_rf, validset$classe)$overall[1]
```

**2) Generallized Boosting Model:**

```{r}
# fit model using gbm
set.seed(3456)
controlgbm <- trainControl(method="cv",number=5,verboseIter=FALSE)
mod_gbm <- train(classe ~ ., distribution="multinomial",data = trainset, method = "gbm",trControl=controlgbm,verbose=FALSE)
mod_gbm$finalModel

#predict on validation dataset
pred_gbm <- predict(mod_gbm, validset)

#Accuracy on validation dataset
confusionMatrix(pred_gbm, validset$classe)
confusionMatrix(pred_gbm, validset$classe)$overall[1]
```


**3) Support Vector Machine:**

```{r}
# fit model using svm
set.seed(4567)
mod_svm <- svm(classe ~ ., data = trainset)

#predict on validation dataset
pred_svm <- predict(mod_svm, validset)

#Accuracy on validation dataset
confusionMatrix(pred_svm, validset$classe)
confusionMatrix(pred_svm, validset$classe)$overall[1]
```

***Compare Accuracy***
```{r}
accuracy_rf <- confusionMatrix(pred_rf, validset$classe)$overall[1]
accuracy_gbm <- confusionMatrix(pred_gbm, validset$classe)$overall[1]
accuracy_svm <- confusionMatrix(pred_svm, validset$classe)$overall[1]
accuracy_compare <- data.frame(Algorithm = c("Random Forest", "Generalized Boosting Model", "Support Vector Machine"), Accuracy = c(accuracy_rf, accuracy_gbm, accuracy_svm))
accuracy_compare
```
We can see that Random Rorest has the highest accuracy rate of 99.67%, followed by Generalized Boosting Model of 98.47%, the last one Support Vetor Machine has the lowest the accuracy rate of 94.54%.

***Apply Prediction Model***

Apply the random forest model on testing data, the answers are:

```{r}
predTesting <- predict(mod_rf, testing)
predTesting
```

